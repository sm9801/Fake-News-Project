{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2598d92c",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Apply sentiment analysis\n",
    "- news article -> model -> { fakeNews: false, sentiment: 0.23 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4b6e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /Users/samuelseokyukim/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/samuelseokyukim/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('subjectivity')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee61cd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('news.csv')\n",
    "\n",
    "# Get shape and head\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04c17cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    FAKE\n",
       "1    FAKE\n",
       "2    REAL\n",
       "3    FAKE\n",
       "4    REAL\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels\n",
    "labels = df.label\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07aaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['text'], labels, test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a34330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through test set and clean each article\n",
    "\n",
    "cleaned_articles = []\n",
    "\n",
    "for document in x_test :\n",
    "    # Remove all the special characters\n",
    "    noSpecial = re.sub(r'\\W', ' ', document)\n",
    "\n",
    "    # Remove all single characters\n",
    "    #noSingle = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', noSpecial)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    #noSingleFromStart = re.sub(r'\\^[a-zA-Z]\\s+', ' ', noSingle) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    noMultiSpace = re.sub(r'\\s+', ' ', noSpecial, flags = re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    noPrefixed = re.sub(r'^b\\s+', '', noMultiSpace)\n",
    "\n",
    "    # Convert to Lowercase\n",
    "    lowercased = noPrefixed.lower()\n",
    "    \n",
    "    cleaned_articles.append(lowercased)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c7480b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_instances = 100\\n\\n# Gather a list of subjective words and a list of objective words\\nsubj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories = 'subj')[:n_instances]]\\nobj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories = 'obj')[:n_instances]]\\nlen(subj_docs), len(obj_docs)\\n\\n# Split subjective and objective instances \\ntrain_subj_docs = subj_docs[:80]\\ntest_subj_docs = subj_docs[80:100]\\ntrain_obj_docs = obj_docs[:80]\\ntest_obj_docs = obj_docs[80:100]\\ntraining_docs = train_subj_docs + train_obj_docs\\ntesting_docs = test_subj_docs + test_obj_docs\\n\\n# Initialize sentiment analyzer\\nsentim_analyzer = SentimentAnalyzer()\\nall_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\\n\\n# Unigram word features handle negation\\nunigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq = 4)\\nprint(len(unigram_feats))\\nsentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\\n\\n# Apply features to obtain a feature-value representation\\ntraining_set = sentim_analyzer.apply_features(training_docs)\\ntest_set = sentim_analyzer.apply_features(testing_docs)\\n\\n# Train the classifier on the training_set\\ntrainer = NaiveBayesClassifier.train\\nclassifier = sentim_analyzer.train(trainer, training_set)\\n\\n# Output evaluation results\\nfor key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\\n    print('{0}: {1}'.format(key, value))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another method of applying sentiment analysis (unfinished)\n",
    "\n",
    "\"\"\"\n",
    "n_instances = 100\n",
    "\n",
    "# Gather a list of subjective words and a list of objective words\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories = 'subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories = 'obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)\n",
    "\n",
    "# Split subjective and objective instances \n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs + train_obj_docs\n",
    "testing_docs = test_subj_docs + test_obj_docs\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "\n",
    "# Unigram word features handle negation\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq = 4)\n",
    "print(len(unigram_feats))\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "\n",
    "# Apply features to obtain a feature-value representation\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "\n",
    "# Train the classifier on the training_set\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "# Output evaluation results\n",
    "for key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223084f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a list of strings and pandas dataframe\n",
    "# Loop through each element in list or dataframe \n",
    "# Applies sentiment analysis\n",
    "def sentiment_analysis(articles_list) :\n",
    "    \n",
    "    for article in articles_list :\n",
    "        \n",
    "        # Initialize sentiment intensity analyzer\n",
    "        sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Print(article)\n",
    "        results = sentiment_analyzer.polarity_scores(article)\n",
    "        print('Sentiment analysis results: ')\n",
    "        \n",
    "        for k in sorted(results) :\n",
    "            print('{0}: {1}, '.format(k, results[k]), end = '')\n",
    "        print()\n",
    "        \n",
    "        print(\"Article was rated as\", results['neg'] * 100, \"% Negative\")\n",
    "        print(\"Article was rated as\", results['neu'] * 100, \"% Neutral\")\n",
    "        print(\"Article was rated as\", results['pos'] * 100, \"% Positive\")\n",
    "        print(\"Overall rating of the article:\", end = ' ')\n",
    "        \n",
    "        # Determine if an article is overall positive, negative, or neutral\n",
    "        if results['compound'] >= 0.05 :\n",
    "            print(\"Positive\")\n",
    " \n",
    "        elif results['compound'] <= - 0.05 :\n",
    "            print(\"Negative\")\n",
    " \n",
    "        else :\n",
    "            print(\"Neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0f5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n",
    "\n",
    "# Fit and transform train set, transform test set\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test = tfidf_vectorizer.transform(cleaned_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dee6269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test: 92.58%\n"
     ]
    }
   ],
   "source": [
    "# Initialize PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier(max_iter = 50)\n",
    "pac.fit(tfidf_train, y_train)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "y_pred = pac.predict(tfidf_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of test: {round(score * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54104590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[585,  44],\n",
       "       [ 50, 588]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build confusion matrix: [[true positive, false negative], [false positive, true negative]]\n",
    "confusion_matrix(y_test, y_pred, labels = ['REAL', 'FAKE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ddafff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: REAL\n",
      "Accuracy of the results: 100.0%\n",
      "Sentiment analysis results: \n",
      "compound: -0.6524, neg: 0.075, neu: 0.866, pos: 0.059, \n",
      "Article was rated as 7.5 % Negative\n",
      "Article was rated as 86.6 % Neutral\n",
      "Article was rated as 5.8999999999999995 % Positive\n",
      "Overall rating of the article: Negative\n"
     ]
    }
   ],
   "source": [
    "# Test a random article from the Wall Street Journal\n",
    "expected_y = ['REAL']\n",
    "x_test_1 = ['Fishing is never an easy way to make a living, but it’s even harder when the federal bureaucracy can put you out of business on a whim. Herring fishermen are fighting back, with potentially large implications for the administrative state. In 2020 the National Marine Fisheries Service (NMFS) decreed that Atlantic herring fishermen must include a human monitor to ensure compliance with catch limits. The fishermen must pay for the monitor, though many are small, independent operators. By the fisheries service’s own estimate, its mandate costs each boat $710 a day. Depending on the catch, this is often more than the captains make on a trip. Even the service acknowledges that the monitoring cost will reduce herring operations’ annual returns by 20%. Enter the Cause of Action Institute, which sued the Commerce Department (which oversees the NMFS) in 2020 on behalf of fishermen in New Jersey. The suit claims Congress never authorized the bureaucratic cost-shift to the industry. The 1976 Magnuson-Stevens Act governs fishing in federal waters and allows the fisheries service to impose monitors. But nowhere in the law does Congress give the service power to force herring fishermen to fund the program. Congress has authorized industry-funded monitoring in other, specific contexts—including for certain North Pacific fisheries, and for foreign vessels. It did not do the same here. The fisheries service argues it has the legal right to impose the costs because the law is silent on the matter. The government points to language authorizing it to take steps that are “necessary and appropriate” to manage fisheries. Put another way, unless Congress explicitly prohibits an action, an agency can proceed. The case has moved up the appellate chain, and in August a divided panel of the D.C. Circuit Court of Appeals ruled for the government. Judges Sri Srinivasan and Judith Rogers cited the Supreme Court’s Chevron deference standard, finding that the law “through its silence, leaves room for agency discretion.” But Judge Justin Walker noted in dissent that this is a principle with no limit. He mused whether the fisheries service could find it “necessary” for fishermen to drive their federal monitors back and forth to the office to save on government gas bills, or whether it could demand fishermen finance other bureaucratic costs. The decision removes one of the few practical constraints on regulatory excess: a lack of resources. If government can write rules and require their targets to pay the costs without explicit Congressional approval, there will be no limit on bureaucratic discretion. The herring fishermen are asking the Supreme Court to hear the case and are represented by former Solicitor General Paul Clement. In West Virginia v. EPA this year, the Court used its major questions doctrine to rein in egregious regulatory overreach. But it has left Chevron as a largely unchecked license for regulators who can still do great harm without Congressional assent. Silence shouldn’t be a bureaucratic license to wreck livelihoods.']\n",
    "tfidf_test_1 = tfidf_vectorizer.transform(x_test_1)\n",
    "actual_y = pac.predict(tfidf_test_1)\n",
    "score_1 = accuracy_score(expected_y, actual_y)\n",
    "\n",
    "if expected_y == actual_y :\n",
    "    print(\"Results:\", actual_y[0])\n",
    "\n",
    "elif expected_y == ['FAKE'] and actual_y == ['REAL'] :\n",
    "    print(\"Results are false positive: news is considered real when it isn't\")\n",
    "\n",
    "elif expected_y == ['REAL'] and actual_y == ['FAKE'] :\n",
    "    print(\"Results are false negative: news is considered fake when it isn't\")\n",
    "\n",
    "print(f'Accuracy of the results: {round(score_1 * 100, 2)}%')\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_analysis(x_test_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c97ec38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix of the random article from the Wall Street Journal\n",
    "confusion_matrix(expected_y, actual_y, labels = ['REAL', 'FAKE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95d29b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are false negative: news is considered fake when it isn't\n",
      "Accuracy of the results: 0.0%\n",
      "Sentiment analysis results: \n",
      "compound: 0.8302, neg: 0.021, neu: 0.889, pos: 0.091, \n",
      "Article was rated as 2.1 % Negative\n",
      "Article was rated as 88.9 % Neutral\n",
      "Article was rated as 9.1 % Positive\n",
      "Overall rating of the article: Positive\n"
     ]
    }
   ],
   "source": [
    "# Test a random article from CNN\n",
    "expected_y_2 = ['REAL']\n",
    "x_test_2 = [\"US stocks exploded higher Wednesday. The Dow is now more than 20% above its 52-week low, which puts it in a new bull market. Federal Reserve chair Jerome Powell strongly suggested that the central bank is ready to slow its pace of interest rate hikes. Powell noted that the Fed is still concerned about inflation but that it also does not want to jeopardize the health of the labor market and broader economy either. Wednesday's rally helped push the markets to their second straight month of solid returns. The three major indexes wrapped up November with gains between 4% and 6%. The Dow soared more than 735 points, or 2.2% The S&P 500 rose 3.1%. The Nasdaq Composite shot up 4.4%. As stocks settle after the trading day, levels might still change slightly.\"]\n",
    "tfidf_test_2 = tfidf_vectorizer.transform(x_test_2)\n",
    "actual_y_2 = pac.predict(tfidf_test_2)\n",
    "score_2 = accuracy_score(expected_y_2, actual_y_2)\n",
    "\n",
    "if expected_y_2 == actual_y_2 :\n",
    "    print(\"Results:\", actual_y_2[0])\n",
    "\n",
    "elif expected_y_2 == ['FAKE'] and actual_y_2 == ['REAL'] :\n",
    "    print(\"Results are false positive: news is considered real when it isn't\")\n",
    "\n",
    "elif expected_y_2 == ['REAL'] and actual_y_2 == ['FAKE'] :\n",
    "    print(\"Results are false negative: news is considered fake when it isn't\")\n",
    "\n",
    "print(f'Accuracy of the results: {round(score_2 * 100, 2)}%')\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_analysis(x_test_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75c32068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have a false negative. Confusion matrix of the random article \n",
    "confusion_matrix(expected_y_2, actual_y_2, labels = ['REAL', 'FAKE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3e42ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: FAKE\n",
      "Accuracy of the results: 100.0%\n",
      "Sentiment analysis results: \n",
      "compound: -0.1036, neg: 0.107, neu: 0.775, pos: 0.118, \n",
      "Article was rated as 10.7 % Negative\n",
      "Article was rated as 77.5 % Neutral\n",
      "Article was rated as 11.799999999999999 % Positive\n",
      "Overall rating of the article: Negative\n"
     ]
    }
   ],
   "source": [
    "# Test a random fake news article from the onion\n",
    "expected_y_3 = ['FAKE']\n",
    "x_test_3 = ['NEW YORK—Emphasizing that the researchers were by no means happy about these results, an unfortunate study published by Columbia University this week found that abusing restaurant waitstaff is the secret to living a longer, happier life. “After tracking thousands of individuals over a period of two decades, we can say with regrettable certainty that treating food service workers like subhuman garbage is the single most important factor in enhancing the length and quality of a human lifespan,” said the study’s lead author Dr. Elizabeth Mitran, who reluctantly explained that just 30 seconds of screaming at a waiter for slow entrées carries greater benefits than an hour of vigorous exercise.“We can’t in good conscience endorse this behavior, but every waiter you make cry adds another year to your life.”']\n",
    "tfidf_test_3 = tfidf_vectorizer.transform(x_test_3)\n",
    "actual_y_3 = pac.predict(tfidf_test_3)\n",
    "score_3 = accuracy_score(expected_y_3, actual_y_3)\n",
    "\n",
    "if expected_y_3 == actual_y_3 :\n",
    "    print(\"Results:\", actual_y_3[0])\n",
    "\n",
    "elif expected_y_3 == ['FAKE'] and actual_y_3 == ['REAL'] :\n",
    "    print(\"Results are false positive: news is considered real when it isn't\")\n",
    "\n",
    "elif expected_y_3 == ['REAL'] and actual_y_3 == ['FAKE'] :\n",
    "    print(\"Results are false negative: news is considered fake when it isn't\")\n",
    "\n",
    "print(f'Accuracy of the results: {round(score_3 * 100, 2)}%')\n",
    "\n",
    "# Apply sentiment analysis\n",
    "sentiment_analysis(x_test_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73fcfa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix of the fake news article\n",
    "confusion_matrix(expected_y_3, actual_y_3, labels = ['REAL', 'FAKE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ddf12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
